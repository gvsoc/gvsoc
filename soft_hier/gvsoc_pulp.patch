diff --git a/pulp/floonoc/floonoc.cpp b/pulp/floonoc/floonoc.cpp
index 668a863..5146e08 100644
--- a/pulp/floonoc/floonoc.cpp
+++ b/pulp/floonoc/floonoc.cpp
@@ -36,6 +36,16 @@ FlooNoc::FlooNoc(vp::ComponentConf &config)
     this->dim_x = get_js_config()->get_int("dim_x");
     this->dim_y = get_js_config()->get_int("dim_y");
     this->router_input_queue_size = get_js_config()->get_int("router_input_queue_size");
+    this->atomics = get_js_config()->get_int("atomics");
+    this->collective = get_js_config()->get_int("collective");
+    this->edge_node_alias = get_js_config()->get_int("edge_node_alias");
+    this->edge_node_alias_start_bit = get_js_config()->get_int("edge_node_alias_start_bit");
+    this->interleave_enable = get_js_config()->get_int("interleave_enable");
+    this->interleave_region_base = get_js_config()->get_int("interleave_region_base");
+    this->interleave_region_size = get_js_config()->get_int("interleave_region_size");
+    this->interleave_granularity = get_js_config()->get_int("interleave_granularity");
+    this->interleave_bit_start = get_js_config()->get_int("interleave_bit_start");
+    this->interleave_bit_width = get_js_config()->get_int("interleave_bit_width");
 
     // Reserve the array for the target. We may have one target at each node.
     this->targets.resize(this->dim_x * this->dim_y);
@@ -71,7 +81,7 @@ FlooNoc::FlooNoc(vp::ComponentConf &config)
             // this array indexed by the position
             this->targets[this->entries[id].y * this->dim_x + this->entries[id].x] = itf;
 
-            this->trace.msg(vp::Trace::LEVEL_DEBUG, "Adding target (name: %s, base: 0x%x, size: 0x%x, x: %d, y: %d)\n",
+            this->trace.msg(vp::Trace::LEVEL_DEBUG, "Adding target (name: %s, base: 0x%llx, size: 0x%llx, x: %d, y: %d)\n",
                 mapping.first.c_str(), this->entries[id].base, this->entries[id].size, this->entries[id].x, this->entries[id].y);
 
             id++;
@@ -117,10 +127,25 @@ FlooNoc::FlooNoc(vp::ComponentConf &config)
 // - When a router gets a synchronous response
 // - When an interface receives a call to the response callback
 // In both cases, the requests is accounted on the initiator burst, in the network interface
+void process_collective_operations(vp::IoReq *parent, vp::IoReq *req);
 void FlooNoc::handle_request_end(vp::IoReq *req)
 {
-    NetworkInterface *ni = *(NetworkInterface **)req->arg_get(FlooNoc::REQ_DEST_NI);
-    ni->handle_response(req);
+    if (*req->arg_get(FlooNoc::REQ_PARENT) != NULL)
+    {
+        vp::IoReq * parent = *(vp::IoReq **)req->arg_get(FlooNoc::REQ_PARENT);
+        process_collective_operations(parent, req);
+        delete req->get_data();
+        delete req;
+        parent->set_int(FlooNoc::REQ_PEND_KIDS, parent->get_int(FlooNoc::REQ_PEND_KIDS) - 1);
+        if (parent->get_int(FlooNoc::REQ_PEND_KIDS) <= 0)
+        {
+            this->handle_request_end(parent);
+        }
+    } else
+    {
+        NetworkInterface *ni = *(NetworkInterface **)req->arg_get(FlooNoc::REQ_DEST_NI);
+        ni->handle_response(req);
+    }
 }
 
 
@@ -196,3 +221,115 @@ extern "C" vp::Component *gv_new(vp::ComponentConf &config)
 {
     return new FlooNoc(config);
 }
+
+
+/****************************************************
+*                   FP16 Utilities                  *
+****************************************************/
+
+typedef union {
+    float f;
+    struct {
+        uint32_t mantissa : 23;
+        uint32_t exponent : 8;
+        uint32_t sign : 1;
+    } parts;
+} FloatBits;
+
+typedef uint16_t fp16;
+
+// Convert float to FP16 (half-precision)
+fp16 float_to_fp16(float value) {
+    FloatBits floatBits;
+    floatBits.f = value;
+
+    uint16_t sign = floatBits.parts.sign << 15;
+    int32_t exponent = floatBits.parts.exponent - 127 + 15; // adjust bias from 127 to 15
+    uint32_t mantissa = floatBits.parts.mantissa >> 13;     // reduce to 10 bits
+
+    if (exponent <= 0) {
+        if (exponent < -10) return sign;   // too small
+        mantissa = (floatBits.parts.mantissa | 0x800000) >> (1 - exponent);
+        return sign | mantissa;
+    } else if (exponent >= 0x1F) {
+        return sign | 0x7C00;  // overflow to infinity
+    }
+    return sign | (exponent << 10) | mantissa;
+}
+
+// Convert FP16 to float
+float fp16_to_float(fp16 value) {
+    FloatBits floatBits;
+    floatBits.parts.sign = (value >> 15) & 0x1;
+    int32_t exponent = (value >> 10) & 0x1F;
+    floatBits.parts.exponent = (exponent == 0) ? 0 : exponent + 127 - 15;
+    floatBits.parts.mantissa = (value & 0x3FF) << 13;
+    return floatBits.f;
+}
+
+void process_collective_operations(vp::IoReq *parent, vp::IoReq *req)
+{
+    int collective_type = parent->get_int(FlooNoc::REQ_COLL_TYPE);
+
+    if (collective_type == 1)
+    {
+        // this->trace.msg(vp::Trace::LEVEL_DEBUG, "[BroadCast]\n");
+
+    } else if (collective_type == 2){
+        // this->trace.msg(vp::Trace::LEVEL_DEBUG, "[Reduction ADD UINT16]\n");
+        //Execute reduction
+        uint16_t * dst = (uint16_t *) parent->get_data();
+        uint16_t * src = (uint16_t *) req->get_data();
+        for (int i = 0; i < (req->get_size()/sizeof(uint16_t)); ++i)
+        {
+            dst[i] = dst[i] + src[i];
+        }
+    } else if (collective_type == 3){
+        // this->trace.msg(vp::Trace::LEVEL_DEBUG, "[Reduction ADD INT16]\n");
+        //Execute reduction
+        int16_t * dst = (int16_t *) parent->get_data();
+        int16_t * src = (int16_t *) req->get_data();
+        for (int i = 0; i < (req->get_size()/sizeof(int16_t)); ++i)
+        {
+            dst[i] = dst[i] + src[i];
+        }
+    } else if (collective_type == 4){
+        // this->trace.msg(vp::Trace::LEVEL_DEBUG, "[Reduction ADD FP16]\n");
+        //Execute reduction
+        fp16 * dst = (fp16 *) parent->get_data();
+        fp16 * src = (fp16 *) req->get_data();
+        for (int i = 0; i < (req->get_size()/sizeof(fp16)); ++i)
+        {
+            dst[i] = float_to_fp16(fp16_to_float(dst[i]) + fp16_to_float(src[i]));
+        }
+    } else if (collective_type == 5){
+        // this->trace.msg(vp::Trace::LEVEL_DEBUG, "[Reduction MAX UINT16]\n");
+        //Execute reduction
+        uint16_t * dst = (uint16_t *) parent->get_data();
+        uint16_t * src = (uint16_t *) req->get_data();
+        for (int i = 0; i < (req->get_size()/sizeof(uint16_t)); ++i)
+        {
+            dst[i] = dst[i] > src[i] ? dst[i] : src[i];
+        }
+    } else if (collective_type == 6){
+        // this->trace.msg(vp::Trace::LEVEL_DEBUG, "[Reduction MAX INT16]\n");
+        //Execute reduction
+        int16_t * dst = (int16_t *) parent->get_data();
+        int16_t * src = (int16_t *) req->get_data();
+        for (int i = 0; i < (req->get_size()/sizeof(int16_t)); ++i)
+        {
+            dst[i] = dst[i] > src[i] ? dst[i] : src[i];
+        }
+    } else if (collective_type == 7){
+        // this->trace.msg(vp::Trace::LEVEL_DEBUG, "[Reduction MAX FP16]\n");
+        //Execute reduction
+        fp16 * dst = (fp16 *) parent->get_data();
+        fp16 * src = (fp16 *) req->get_data();
+        for (int i = 0; i < (req->get_size()/sizeof(fp16)); ++i)
+        {
+            dst[i] = float_to_fp16(fp16_to_float(dst[i]) > fp16_to_float(src[i]) ? fp16_to_float(dst[i]) : fp16_to_float(src[i]));
+        }
+    } else {
+        // this->trace.fatal("Invalid collective operation: %d\n", collective_type);
+    }
+}
diff --git a/pulp/floonoc/floonoc.hpp b/pulp/floonoc/floonoc.hpp
index 644696a..4c675ac 100644
--- a/pulp/floonoc/floonoc.hpp
+++ b/pulp/floonoc/floonoc.hpp
@@ -95,7 +95,15 @@ public:
     static constexpr int REQ_DEST_Y = 4;      // Y coordinate of the destination target
     static constexpr int REQ_ROUTER = 5;      // When a request is stalled, this gives the router where to grant it
     static constexpr int REQ_QUEUE = 6;       // When a request is stalled, this gives the queue where to grant it
-    static constexpr int REQ_NB_ARGS = 7;     // Number of request data required by this model
+    static constexpr int REQ_SRC_X = 7;       // X coordinate of the source target
+    static constexpr int REQ_SRC_Y = 8;       // Y coordinate of the source target
+    static constexpr int REQ_PARENT = 9;      // [Collective Only] If it is a request from a router internally
+    static constexpr int REQ_COLL_TYPE = 10;      // [Collective Only] If it is a request from a router internally
+    static constexpr int REQ_ROW_MASK = 11;   // [Collective Only] The row mask for collecitive primitives
+    static constexpr int REQ_COL_MASK = 12;   // [Collective Only] The col mask for collecitive primitives
+    static constexpr int REQ_PEND_KIDS = 13;   // [Collective Only] How many kids to be waited for response
+    static constexpr int REQ_MOMENTUM = 14;      // [Collective Only] Tree direction
+    static constexpr int REQ_NB_ARGS = 15;    // Number of request data required by this model
 
     // The following constants gives the index in the queue array of the queue associated to each direction
     static constexpr int DIR_RIGHT = 0;
@@ -104,10 +112,40 @@ public:
     static constexpr int DIR_DOWN = 3;
     static constexpr int DIR_LOCAL = 4;
 
+    // [Collective Only]
+    static constexpr int MOMENTUM_RIGHT = 0;
+    static constexpr int MOMENTUM_LEFT  = 1;
+    static constexpr int MOMENTUM_UP    = 2;
+    static constexpr int MOMENTUM_DOWN  = 3;
+    static constexpr int MOMENTUM_ZERO  = 4;
+
     // Width in bytes of the noc. This is used to split incoming bursts into internal requests of
     // this width so that the bandwidth corresponds to the width.
     uint64_t width;
 
+    // Whether Support Atomics
+    bool atomics;
+
+    // Whether Support Collective
+    bool collective;
+
+    // Whether Support HBM node aliasing
+    uint64_t edge_node_alias;
+    uint64_t edge_node_alias_start_bit;
+
+    // Whether Support Interleaving
+    uint64_t interleave_enable;
+    uint64_t interleave_region_base;
+    uint64_t interleave_region_size;
+    uint64_t interleave_granularity;
+    uint64_t interleave_bit_start;
+    uint64_t interleave_bit_width;
+
+    // X dimension of the network. This includes both routers but also targets on the edges
+    int dim_x;
+    // Y dimension of the network. This includes both routers but also targets on the edges
+    int dim_y;
+
 private:
     // Callback called when a target request is asynchronously granted after a denied error was
     // reported
@@ -121,10 +159,6 @@ private:
     // Set of memory-mapped entries, with one for each target. They give information about each
     // target (base address, size, position)
     std::vector<Entry> entries;
-    // X dimension of the network. This includes both routers but also targets on the edges
-    int dim_x;
-    // Y dimension of the network. This includes both routers but also targets on the edges
-    int dim_y;
     // SIze of the routers input queues. Pushing more requests than this size will block the
     // output queue of the sender.
     int router_input_queue_size;
diff --git a/pulp/floonoc/floonoc.py b/pulp/floonoc/floonoc.py
index 2aa365c..8c0dbb1 100644
--- a/pulp/floonoc/floonoc.py
+++ b/pulp/floonoc/floonoc.py
@@ -47,7 +47,9 @@ class FlooNoc2dMesh(gvsoc.systree.Component):
         before the source output queue is stalled.
     """
     def __init__(self, parent: gvsoc.systree.Component, name, width: int,
-            dim_x: int, dim_y:int, ni_outstanding_reqs: int=8, router_input_queue_size: int=2):
+            dim_x: int, dim_y:int, ni_outstanding_reqs: int=8, router_input_queue_size: int=2, atomics: int=0, collective: int=0,
+            edge_node_alias: int=0, edge_node_alias_start_bit: int=48,
+            interleave_enable: int=0, interleave_region_base: int=0, interleave_region_size: int=0, interleave_granularity: int=0, interleave_bit_start: int=0, interleave_bit_width: int=0):
         super(FlooNoc2dMesh, self).__init__(parent, name)
 
         self.add_sources([
@@ -64,6 +66,16 @@ class FlooNoc2dMesh(gvsoc.systree.Component):
         self.add_property('dim_x', dim_x)
         self.add_property('dim_y', dim_y)
         self.add_property('router_input_queue_size', router_input_queue_size)
+        self.add_property('atomics', atomics)
+        self.add_property('collective', collective)
+        self.add_property('interleave_enable', interleave_enable)
+        self.add_property('interleave_region_base', interleave_region_base)
+        self.add_property('interleave_region_size', interleave_region_size)
+        self.add_property('interleave_granularity', interleave_granularity)
+        self.add_property('interleave_bit_start', interleave_bit_start)
+        self.add_property('interleave_bit_width', interleave_bit_width)
+        self.add_property('edge_node_alias', edge_node_alias)
+        self.add_property('edge_node_alias_start_bit', edge_node_alias_start_bit)
 
     def __add_mapping(self, name: str, base: int, size: int, x: int, y: int):
         self.get_property('mappings')[name] =  {'base': base, 'size': size, 'x': x, 'y': y}
diff --git a/pulp/floonoc/floonoc_network_interface.cpp b/pulp/floonoc/floonoc_network_interface.cpp
index 4124c25..08e15ad 100644
--- a/pulp/floonoc/floonoc_network_interface.cpp
+++ b/pulp/floonoc/floonoc_network_interface.cpp
@@ -90,7 +90,7 @@ void NetworkInterface::fsm_handler(vp::Block *__this, vp::ClockEvent *event)
         // In case the burst is being handled for the first time, initialize the current burst
         if (_this->pending_burst_size == 0)
         {
-            _this->trace.msg(vp::Trace::LEVEL_DEBUG, "Start handling burst (burst: %p, base: 0x%x, size: 0x%x, is_write: %d)\n",
+            _this->trace.msg(vp::Trace::LEVEL_DEBUG, "Start handling burst (burst: %p, base: 0x%llx, size: 0x%x, is_write: %d)\n",
                 burst, burst->get_addr(), burst->get_size(), burst->get_is_write());
 
             // By default, we consider the whole burst as valid. In one of the burst request is\
@@ -111,6 +111,59 @@ void NetworkInterface::fsm_handler(vp::Block *__this, vp::ClockEvent *event)
         // Get base from current burst
         uint64_t base = _this->pending_burst_base;
 
+        // We apply the HBM node aliasing
+        if (_this->noc->edge_node_alias > 1)
+        {
+            Entry *test_entry = _this->noc->get_entry(base, 1);
+            if (test_entry == NULL) _this->trace.fatal("[NoC Edge Alias] Cannot find entry for base 0x%llx \n",base);
+            if (test_entry->x == 0)
+            {
+                //west edge
+                uint64_t node_id = _this->y - 1;
+                uint64_t alias_offset = 1;
+                alias_offset = alias_offset << _this->noc->edge_node_alias_start_bit;
+                base += alias_offset * (node_id % _this->noc->edge_node_alias);
+                _this->trace.msg(vp::Trace::LEVEL_DEBUG, "[NoC Edge Alias] West edge node %d alias to 0x%llx \n", node_id, base);
+            } else if (test_entry->x == _this->noc->dim_x - 1)
+            {
+                //east edge
+                uint64_t node_id = _this->y - 1;
+                uint64_t alias_offset = 1;
+                alias_offset = alias_offset << _this->noc->edge_node_alias_start_bit;
+                base += alias_offset * (node_id % _this->noc->edge_node_alias);
+                _this->trace.msg(vp::Trace::LEVEL_DEBUG, "[NoC Edge Alias] East edge node %d alias to 0x%llx \n", node_id, base);
+            } else if (test_entry->y == 0)
+            {
+                //south edge
+                uint64_t node_id = _this->x - 1;
+                uint64_t alias_offset = 1;
+                alias_offset = alias_offset << _this->noc->edge_node_alias_start_bit;
+                base += alias_offset * (node_id % _this->noc->edge_node_alias);
+                _this->trace.msg(vp::Trace::LEVEL_DEBUG, "[NoC Edge Alias] South edge node %d alias to 0x%llx \n", node_id, base);
+            } else if (test_entry->y == _this->noc->dim_y - 1)
+            {
+                //north edge
+                uint64_t node_id = _this->x - 1;
+                uint64_t alias_offset = 1;
+                alias_offset = alias_offset << _this->noc->edge_node_alias_start_bit;
+                base += alias_offset * (node_id % _this->noc->edge_node_alias);
+                _this->trace.msg(vp::Trace::LEVEL_DEBUG, "[NoC Edge Alias] North edge node %d alias to 0x%llx \n", node_id, base);
+            }
+
+        }
+
+        //If support interleaving
+        if (_this->noc->interleave_enable && base >= _this->noc->interleave_region_base && base < _this->noc->interleave_region_base + _this->noc->interleave_region_size)
+        {
+            uint32_t mask = ((1 << _this->noc->interleave_bit_width) - 1);
+            uint32_t range1 = (base >> _this->noc->interleave_granularity) & mask;
+            uint32_t range2 = (base >> _this->noc->interleave_bit_start) & mask;
+            base &= ~(mask << _this->noc->interleave_granularity);
+            base &= ~(mask << _this->noc->interleave_bit_start);
+            base |= (range1 << _this->noc->interleave_bit_start);
+            base |= (range2 << _this->noc->interleave_granularity);
+        }
+
         // Size must be at max the noc width to respect the bandwidth
         uint64_t size = std::min(_this->noc->width, _this->pending_burst_size);
         // And must not cross a page to fall into one target
@@ -130,6 +183,32 @@ void NetworkInterface::fsm_handler(vp::Block *__this, vp::ClockEvent *event)
         req->set_size(size);
         req->set_data(_this->pending_burst_data);
         req->set_is_write(burst->get_is_write());
+        if (_this->noc->atomics)
+        {
+            req->set_opcode(burst->get_opcode());
+            req->set_second_data(burst->get_second_data());
+        }
+
+        //Deal with collective primitives
+        *req->arg_get(FlooNoc::REQ_PARENT) = (void *)0;
+        *req->arg_get(FlooNoc::REQ_COLL_TYPE) = (void *)0;
+        *req->arg_get(FlooNoc::REQ_ROW_MASK) = (void *)0;
+        *req->arg_get(FlooNoc::REQ_COL_MASK) = (void *)0;
+        *req->arg_get(FlooNoc::REQ_PEND_KIDS) = (void *)0;
+        *req->arg_get(FlooNoc::REQ_MOMENTUM) = (void *)FlooNoc::MOMENTUM_ZERO;
+        if (_this->noc->collective)
+        {
+            uint8_t collective_type = burst->get_payload()[0];
+            _this->trace.msg(vp::Trace::LEVEL_DEBUG, "[Collective] preload[0] of burst %d\n",collective_type);
+            if (collective_type>0 && collective_type<8)
+            {
+                uint8_t row_mask = burst->get_payload()[1];
+                uint8_t col_mask = burst->get_payload()[2];
+                req->set_int(FlooNoc::REQ_COLL_TYPE, collective_type);
+                req->set_int(FlooNoc::REQ_ROW_MASK, row_mask);
+                req->set_int(FlooNoc::REQ_COL_MASK, col_mask);
+            }
+        }
 
         // Get the target entry corresponding to the current base
         Entry *entry = _this->noc->get_entry(base, size);
@@ -137,7 +216,7 @@ void NetworkInterface::fsm_handler(vp::Block *__this, vp::ClockEvent *event)
         {
             // If any request of the burst is invalid because no target was found, make the whole
             // burst invalid.
-            _this->trace.force_warning("No entry found for burst (base: 0x%x, size: 0x%x)",
+            _this->trace.force_warning("No entry found for burst (base: 0x%llx, size: 0x%x)",
                 base, size);
             burst->status = vp::IO_REQ_INVALID;
 
@@ -171,11 +250,13 @@ void NetworkInterface::fsm_handler(vp::Block *__this, vp::ClockEvent *event)
             req->set_addr(base - entry->base);
             *req->arg_get(FlooNoc::REQ_DEST_X) = (void *)(long)entry->x;
             *req->arg_get(FlooNoc::REQ_DEST_Y) = (void *)(long)entry->y;
+            *req->arg_get(FlooNoc::REQ_SRC_X) = (void *)(long)_this->x;
+            *req->arg_get(FlooNoc::REQ_SRC_Y) = (void *)(long)_this->y;
 
             // And forward to the first router which is at the same position as the network
             // interface
-            _this->trace.msg(vp::Trace::LEVEL_DEBUG, "Injecting request to noc (req: %p, base: 0x%x, size: 0x%x, destination: (%d, %d))\n",
-                req, base, size, entry->x, entry->y);
+            _this->trace.msg(vp::Trace::LEVEL_DEBUG, "Injecting request to noc (req: %p, base: 0x%llx, size: 0x%x, op_code: %0d, destination: (%d, %d))\n",
+                req, base, size, req->get_opcode(), entry->x, entry->y);
 
             // Noe that the router may not grant tje request if its input queue is full.
             // In this case we must stall the network interface
@@ -232,7 +313,7 @@ vp::IoReqStatus NetworkInterface::req(vp::Block *__this, vp::IoReq *req)
     uint8_t *data = req->get_data();
     uint64_t size = req->get_size();
 
-    _this->trace.msg(vp::Trace::LEVEL_DEBUG, "Received burst (burst: %p, offset: 0x%x, size: 0x%x, is_write: %d, op: %d)\n",
+    _this->trace.msg(vp::Trace::LEVEL_DEBUG, "Received burst (burst: %p, offset: 0x%llx, size: 0x%x, is_write: %d, op: %d)\n",
         req, offset, size, req->get_is_write(), req->get_opcode());
 
     // Just enqueue it and trigger the FSM which will check if it must be processed now
diff --git a/pulp/floonoc/floonoc_router.cpp b/pulp/floonoc/floonoc_router.cpp
index 50cb4ac..ea9756f 100644
--- a/pulp/floonoc/floonoc_router.cpp
+++ b/pulp/floonoc/floonoc_router.cpp
@@ -21,6 +21,7 @@
 
 #include <vp/vp.hpp>
 #include <vp/itf/io.hpp>
+#include <cstring>
 #include "floonoc.hpp"
 #include "floonoc_router.hpp"
 #include "floonoc_network_interface.hpp"
@@ -42,6 +43,8 @@ Router::Router(FlooNoc *noc, int x, int y, int queue_size)
     {
         this->input_queues[i] = new vp::Queue(this, "input_queue_" + std::to_string(i),
             &this->fsm_event);
+        this->collective_generated_queues[i] = new std::queue<vp::IoReq*>();
+        this->stalled_queues[i] = false;
     }
 }
 
@@ -87,7 +90,8 @@ void Router::fsm_handler(vp::Block *__this, vp::ClockEvent *event)
     for (int i=0; i<5; i++)
     {
         vp::Queue *queue = _this->input_queues[queue_index];
-        if (!queue->empty())
+        // if (!queue->empty())
+        if (queue->size())
         {
             vp::IoReq *req = (vp::IoReq *)queue->head();
 
@@ -100,14 +104,53 @@ void Router::fsm_handler(vp::Block *__this, vp::ClockEvent *event)
             // to go to the destination
             int next_x, next_y;
             _this->get_next_router_pos(to_x, to_y, next_x, next_y);
-            _this->trace.msg(vp::Trace::LEVEL_DEBUG, "Resolved next position (req: %p, next_position: (%d, %d))\n",
-                req, next_x, next_y);
+
+            /**************************************/
+            /*   Deal With Collective Primitives  */
+            /**************************************/
+            if (req->get_int(FlooNoc::REQ_COLL_TYPE))
+            {
+                std::queue<int> analyze_result;
+                //1. analyze
+                _this->collective_analyze(req, &analyze_result, _this->x, _this->y);
+                _this->trace.msg(vp::Trace::LEVEL_DEBUG, "[Collective] analyze_result has %d elements\n",analyze_result.size());
+
+                //2. process
+                if (analyze_result.size() == 0)
+                {
+                    //2.1 No action needed, directly response
+                    _this->noc->handle_request_end(req);
+                    queue->pop();
+                    queue_index += 1; if (queue_index == 5) queue_index = 0;
+                    continue;
+                } else
+                if (analyze_result.size() == 1)
+                {
+                    //2.2 Only one request is needed, modfiy the req accordingly
+                    req->set_int(FlooNoc::REQ_MOMENTUM, analyze_result.front());
+                    to_x = _this->x; to_y = _this->y; next_x = _this->x; next_y = _this->y;
+                    if (analyze_result.front() == FlooNoc::MOMENTUM_RIGHT) {to_x += 1; next_x += 1;}
+                    if (analyze_result.front() == FlooNoc::MOMENTUM_LEFT)  {to_x -= 1; next_x -= 1;}
+                    if (analyze_result.front() == FlooNoc::MOMENTUM_UP)    {to_y += 1; next_y += 1;}
+                    if (analyze_result.front() == FlooNoc::MOMENTUM_DOWN)  {to_y -= 1; next_y -= 1;}
+                    if (analyze_result.front() == FlooNoc::MOMENTUM_ZERO)  {/*Do Nothing*/}
+                    req->set_int(FlooNoc::REQ_DEST_X, to_x);
+                    req->set_int(FlooNoc::REQ_DEST_Y, to_y);
+                } else {
+                    // Generate Kid Requests Accordingly
+                    _this->collective_generate(req, &analyze_result, _this->x, _this->y);
+                    queue->pop();
+                    queue_index += 1; if (queue_index == 5) queue_index = 0;
+                    continue;
+                }
+            }
 
             // In case the request goes to a queue which is stalled, skip it
             // we'll retry later
             int queue_id = _this->get_req_queue(next_x, next_y);
             if (_this->stalled_queues[queue_id])
             {
+                queue_index += 1; if (queue_index == 5) queue_index = 0;
                 continue;
             }
 
@@ -173,14 +216,49 @@ void Router::fsm_handler(vp::Block *__this, vp::ClockEvent *event)
             // Since we removed a request, check in next cycle if there is another one to handle
             _this->fsm_event.enqueue();
 
-            break;
+            // break;
         }
 
         // If we didn't any ready request, try with next queue
-        queue_index += 1;
-        if (queue_index == 5)
+        queue_index += 1; if (queue_index == 5) queue_index = 0;
+    }
+
+    // Deal with collective primitives generated pending requests
+    for (int i = 0; i < 5; ++i)
+    {
+        std::queue<vp::IoReq *> * queue = _this->collective_generated_queues[i];
+        if (queue->size())
         {
-            queue_index = 0;
+            vp::IoReq *req = (vp::IoReq *)queue->front();
+            int to_x = req->get_int(FlooNoc::REQ_DEST_X);
+            int to_y = req->get_int(FlooNoc::REQ_DEST_Y);
+            int next_x, next_y;
+            _this->get_next_router_pos(to_x, to_y, next_x, next_y);
+            int queue_id = _this->get_req_queue(next_x, next_y);
+            if (_this->stalled_queues[queue_id]) continue;
+            queue->pop();
+            if (to_x == _this->x && to_y == _this->y)
+            {
+                _this->send_to_target(req, _this->x, _this->y);
+            }
+            else
+            {
+                Router *router = _this->noc->get_router(next_x, next_y);
+                if (router == NULL)
+                {
+                    _this->send_to_target(req, next_x, next_y);
+                }
+                else
+                {
+                    _this->trace.msg(vp::Trace::LEVEL_DEBUG, "[Collective] Forwarding request to next router (req: %p, next_position: (%d, %d))\n",
+                        req, next_x, next_y);
+                    if (router->handle_request(req, _this->x, _this->y))
+                    {
+                        _this->stalled_queues[queue_id] = true;
+                    }
+                }
+            }
+            _this->fsm_event.enqueue();
         }
     }
 }
@@ -204,7 +282,7 @@ void Router::send_to_target(vp::IoReq *req, int pos_x, int pos_y)
         req->status = result;
         this->noc->handle_request_end(req);
     }
-    else if (vp::IO_REQ_DENIED)
+    else if (result == vp::IO_REQ_DENIED)
     {
         int queue = this->get_req_queue(pos_x, pos_y);
 
@@ -225,7 +303,174 @@ void Router::send_to_target(vp::IoReq *req, int pos_x, int pos_y)
     }
 }
 
+bool check_target(int cur_x, int cur_y, int src_x, int src_y, int row_mask, int col_mask){
+    bool check_x = (src_x & row_mask) == (cur_x & row_mask);
+    bool check_y = (src_y & col_mask) == (cur_y & col_mask);
+    return check_x & check_y;
+}
+
+bool check_momentum(int momentum, int cur_x, int cur_y, int src_x, int src_y, int dim_x, int dim_y, int row_mask, int col_mask){
+    if (momentum == FlooNoc::MOMENTUM_RIGHT)
+    {
+        for (int i = cur_x + 1; i < dim_x; ++i)
+        {
+            if (check_target(i, cur_y, src_x, src_y, row_mask, col_mask))
+            {
+                return true;
+            }
+        }
+        return false;
+    }
+
+    if (momentum == FlooNoc::MOMENTUM_LEFT)
+    {
+        for (int i = cur_x - 1; i >= 0; --i)
+        {
+            if (check_target(i, cur_y, src_x, src_y, row_mask, col_mask))
+            {
+                return true;
+            }
+        }
+        return false;
+    }
+
+    if (momentum == FlooNoc::MOMENTUM_UP)
+    {
+        for (int i = cur_y + 1; i < dim_y; ++i)
+        {
+            if (check_target(cur_x, i, src_x, src_y, row_mask, col_mask))
+            {
+                return true;
+            }
+        }
+        return false;
+    }
+
+    if (momentum == FlooNoc::MOMENTUM_DOWN)
+    {
+        for (int i = cur_y - 1; i >= 0; --i)
+        {
+            if (check_target(cur_x, i, src_x, src_y, row_mask, col_mask))
+            {
+                return true;
+            }
+        }
+        return false;
+    }
+}
+
+const char * get_momentum_name(int momentum){
+    if (momentum == FlooNoc::MOMENTUM_RIGHT) return "Right";
+    if (momentum == FlooNoc::MOMENTUM_LEFT) return "Left";
+    if (momentum == FlooNoc::MOMENTUM_UP) return "Up";
+    if (momentum == FlooNoc::MOMENTUM_DOWN) return "Down";
+    if (momentum == FlooNoc::MOMENTUM_ZERO) return "Zero";
+    return "Unknow";
+}
+
+void Router::collective_analyze(vp::IoReq * req, std::queue<int> * queue, int router_x, int router_y)
+{
+    int src_x = req->get_int(FlooNoc::REQ_SRC_X) - 1;
+    int src_y = req->get_int(FlooNoc::REQ_SRC_Y) - 1;
+    int cur_x = router_x - 1;
+    int cur_y = router_y - 1;
+    int dim_x = this->noc->dim_x - 2;
+    int dim_y = this->noc->dim_y - 2;
+    int row_m = req->get_int(FlooNoc::REQ_ROW_MASK);
+    int col_m = req->get_int(FlooNoc::REQ_COL_MASK);
+    this->trace.msg(vp::Trace::LEVEL_DEBUG, "[Collective] cur_x: %d, cur_y: %d, dim_x: %d, dim_y: %d\n",cur_x, cur_y, dim_x, dim_y);
+    if (req->get_int(FlooNoc::REQ_MOMENTUM) == FlooNoc::MOMENTUM_ZERO)
+    {
+        if (check_target(cur_x,cur_y,src_x,src_y,row_m,col_m)) queue->push(FlooNoc::MOMENTUM_ZERO);
+        if (check_momentum(FlooNoc::MOMENTUM_RIGHT,cur_x,cur_y,src_x,src_y,dim_x,dim_y,row_m,col_m)) queue->push(FlooNoc::MOMENTUM_RIGHT);
+        if (check_momentum(FlooNoc::MOMENTUM_LEFT, cur_x,cur_y,src_x,src_y,dim_x,dim_y,row_m,col_m)) queue->push(FlooNoc::MOMENTUM_LEFT);
+        if (check_momentum(FlooNoc::MOMENTUM_UP,   cur_x,cur_y,src_x,src_y,dim_x,dim_y,row_m,col_m)) queue->push(FlooNoc::MOMENTUM_UP);
+        if (check_momentum(FlooNoc::MOMENTUM_DOWN, cur_x,cur_y,src_x,src_y,dim_x,dim_y,row_m,col_m)) queue->push(FlooNoc::MOMENTUM_DOWN);
+    }
+
+    if (req->get_int(FlooNoc::REQ_MOMENTUM) == FlooNoc::MOMENTUM_RIGHT)
+    {
+        if (check_target(cur_x,cur_y,src_x,src_y,row_m,col_m)) queue->push(FlooNoc::MOMENTUM_ZERO);
+        if (check_momentum(FlooNoc::MOMENTUM_RIGHT,cur_x,cur_y,src_x,src_y,dim_x,dim_y,row_m,col_m)) queue->push(FlooNoc::MOMENTUM_RIGHT);
+        if (check_momentum(FlooNoc::MOMENTUM_UP,   cur_x,cur_y,src_x,src_y,dim_x,dim_y,row_m,col_m)) queue->push(FlooNoc::MOMENTUM_UP);
+        if (check_momentum(FlooNoc::MOMENTUM_DOWN, cur_x,cur_y,src_x,src_y,dim_x,dim_y,row_m,col_m)) queue->push(FlooNoc::MOMENTUM_DOWN);
+    }
+
+    if (req->get_int(FlooNoc::REQ_MOMENTUM) == FlooNoc::MOMENTUM_LEFT)
+    {
+        if (check_target(cur_x,cur_y,src_x,src_y,row_m,col_m)) queue->push(FlooNoc::MOMENTUM_ZERO);
+        if (check_momentum(FlooNoc::MOMENTUM_LEFT, cur_x,cur_y,src_x,src_y,dim_x,dim_y,row_m,col_m)) queue->push(FlooNoc::MOMENTUM_LEFT);
+        if (check_momentum(FlooNoc::MOMENTUM_UP,   cur_x,cur_y,src_x,src_y,dim_x,dim_y,row_m,col_m)) queue->push(FlooNoc::MOMENTUM_UP);
+        if (check_momentum(FlooNoc::MOMENTUM_DOWN, cur_x,cur_y,src_x,src_y,dim_x,dim_y,row_m,col_m)) queue->push(FlooNoc::MOMENTUM_DOWN);
+    }
+
+    if (req->get_int(FlooNoc::REQ_MOMENTUM) == FlooNoc::MOMENTUM_UP)
+    {
+        if (check_target(cur_x,cur_y,src_x,src_y,row_m,col_m)) queue->push(FlooNoc::MOMENTUM_ZERO);
+        if (check_momentum(FlooNoc::MOMENTUM_UP,   cur_x,cur_y,src_x,src_y,dim_x,dim_y,row_m,col_m)) queue->push(FlooNoc::MOMENTUM_UP);
+    }
 
+    if (req->get_int(FlooNoc::REQ_MOMENTUM) == FlooNoc::MOMENTUM_DOWN)
+    {
+        if (check_target(cur_x,cur_y,src_x,src_y,row_m,col_m)) queue->push(FlooNoc::MOMENTUM_ZERO);
+        if (check_momentum(FlooNoc::MOMENTUM_DOWN, cur_x,cur_y,src_x,src_y,dim_x,dim_y,row_m,col_m)) queue->push(FlooNoc::MOMENTUM_DOWN);
+    }
+}
+
+void Router::collective_generate(vp::IoReq * req, std::queue<int> * queue, int router_x, int router_y)
+{
+    int num_req = queue->size();
+    for (int i = 0; i < num_req; ++i)
+    {
+        int momentum = queue->front();
+        this->trace.msg(vp::Trace::LEVEL_DEBUG, "[Collective] Generate New Req in Momentum of %s\n",get_momentum_name(momentum));
+
+        //New request
+        vp::IoReq *kid = new vp::IoReq();
+        kid->init();
+        kid->arg_alloc(FlooNoc::REQ_NB_ARGS);
+        *kid->arg_get(FlooNoc::REQ_DEST_NI) = *req->arg_get(FlooNoc::REQ_DEST_NI);
+        *kid->arg_get(FlooNoc::REQ_DEST_BURST) = *req->arg_get(FlooNoc::REQ_DEST_BURST);
+        *kid->arg_get(FlooNoc::REQ_DEST_BASE) = *req->arg_get(FlooNoc::REQ_DEST_BASE);
+        kid->set_size(req->get_size());
+        uint8_t * data_ptr = new uint8_t[req->get_size()];
+        std::memcpy(data_ptr, req->get_data(), req->get_size());
+        kid->set_data(data_ptr);
+        kid->set_is_write(req->get_is_write());
+        if (this->noc->atomics)
+        {
+            kid->set_opcode(req->get_opcode());
+            kid->set_second_data(req->get_second_data());
+        }
+        *kid->arg_get(FlooNoc::REQ_PARENT) = (void *)req;
+        *kid->arg_get(FlooNoc::REQ_COLL_TYPE) = *req->arg_get(FlooNoc::REQ_COLL_TYPE);
+        *kid->arg_get(FlooNoc::REQ_ROW_MASK) = *req->arg_get(FlooNoc::REQ_ROW_MASK);
+        *kid->arg_get(FlooNoc::REQ_COL_MASK) = *req->arg_get(FlooNoc::REQ_COL_MASK);
+        *kid->arg_get(FlooNoc::REQ_PEND_KIDS) = (void *)0;
+        kid->set_int(FlooNoc::REQ_MOMENTUM, momentum);
+        kid->set_addr(req->get_addr());
+        *kid->arg_get(FlooNoc::REQ_SRC_X) = *req->arg_get(FlooNoc::REQ_SRC_X);
+        *kid->arg_get(FlooNoc::REQ_SRC_Y) = *req->arg_get(FlooNoc::REQ_SRC_Y);
+        int to_x = router_x;
+        int to_y = router_y;
+        if (queue->front() == FlooNoc::MOMENTUM_RIGHT) {to_x += 1;}
+        if (queue->front() == FlooNoc::MOMENTUM_LEFT)  {to_x -= 1;}
+        if (queue->front() == FlooNoc::MOMENTUM_UP)    {to_y += 1;}
+        if (queue->front() == FlooNoc::MOMENTUM_DOWN)  {to_y -= 1;}
+        if (queue->front() == FlooNoc::MOMENTUM_ZERO)  {/*Do Nothing*/}
+        kid->set_int(FlooNoc::REQ_DEST_X, to_x);
+        kid->set_int(FlooNoc::REQ_DEST_Y, to_y);
+
+        //Modify parent
+        req->set_int(FlooNoc::REQ_PEND_KIDS, req->get_int(FlooNoc::REQ_PEND_KIDS) + 1);
+
+        //Push to collective pending queue
+        this->collective_generated_queues[momentum]->push(kid);
+
+        //The next
+        queue->pop();
+    }
+}
 
 void Router::grant(vp::IoReq *req)
 {
@@ -241,23 +486,34 @@ void Router::grant(vp::IoReq *req)
 
 void Router::get_next_router_pos(int dest_x, int dest_y, int &next_x, int &next_y)
 {
-    // Simple algorithm to reach the destination.
-    // We just move on the direction where we find the highest difference.
-    // To be checked on real HW, there is probably a better algorithm to take different paths
-    // depending on the congestion.
-    int x_diff = dest_x - this->x;
-    int y_diff = dest_y - this->y;
-
-    if (std::abs(x_diff) > std::abs(y_diff))
+    // XY routing algorithm
+    int eff_dest_x = dest_x;
+
+    if (dest_x == 0 && dest_y != this->y)
     {
-        next_x = x_diff < 0 ? this->x - 1 : this->x + 1;
+        eff_dest_x = 1;
+    } else
+    if (dest_x == (this->noc->dim_x - 1) && dest_y != this->y)
+    {
+        eff_dest_x = this->noc->dim_x - 2;
+    }
+
+    if (eff_dest_x == this->x && dest_y == this->y)
+    {
+        next_x = this->x;
         next_y = this->y;
     }
-    else
+    else if (eff_dest_x == this->x)
     {
-        next_y = y_diff < 0 ? this->y - 1 : this->y + 1;
         next_x = this->x;
+        next_y = dest_y < this->y ? this->y - 1 : this->y + 1;
+    }
+    else
+    {
+        next_x = eff_dest_x < this->x ? this->x - 1 : this->x + 1;
+        next_y = this->y;
     }
+
 }
 
 
diff --git a/pulp/floonoc/floonoc_router.hpp b/pulp/floonoc/floonoc_router.hpp
index e455f21..b369dd2 100644
--- a/pulp/floonoc/floonoc_router.hpp
+++ b/pulp/floonoc/floonoc_router.hpp
@@ -49,6 +49,8 @@ private:
     static void fsm_handler(vp::Block *__this, vp::ClockEvent *event);
     // Called when a request has reached its destination position and should be sent to a target
     void send_to_target(vp::IoReq *req, int pos_x, int pos_y);
+    // Called when a request has collective operation
+    void handle_collective(vp::IoReq *req, uint8_t collective_type, int pos_x, int pos_y);
     // Get the position of the next router which should handle a request.
     void get_next_router_pos(int dest_x, int dest_y, int &next_x, int &next_y);
     // Get the index of the queue corresponding to a source or destination position
@@ -80,4 +82,8 @@ private:
     // State of the output queues, true if it is stalled and nothing can be sent to it anymore
     // until it is unstalled.
     bool stalled_queues[5];
+    // The pending queue for collective generated requests
+    std::queue<vp::IoReq *> *collective_generated_queues[5];
+    void collective_analyze(vp::IoReq * req, std::queue<int> * queue, int router_x, int router_y);
+    void collective_generate(vp::IoReq * req, std::queue<int> * queue, int router_x, int router_y);
 };
diff --git a/pulp/idma/be/idma_be.cpp b/pulp/idma/be/idma_be.cpp
index 4e3a273..b82eab5 100644
--- a/pulp/idma/be/idma_be.cpp
+++ b/pulp/idma/be/idma_be.cpp
@@ -27,7 +27,8 @@ IDmaBe::IDmaBe(vp::Component *idma, IdmaTransferProducer *me,
     IdmaBeConsumer *loc_be_read, IdmaBeConsumer *loc_be_write,
     IdmaBeConsumer *ext_be_read, IdmaBeConsumer *ext_be_write)
 :   Block(idma, "be"),
-    fsm_event(this, &IDmaBe::fsm_handler)
+    fsm_event(this, &IDmaBe::fsm_handler),
+    transfer_regulation_event(this, &IDmaBe::transfer_regulation_handler)
 {
     // Middle-end and backend protocols will be used later for interaction
     this->me = me;
@@ -59,23 +60,55 @@ IdmaBeConsumer *IDmaBe::get_be_consumer(uint64_t base, uint64_t size, bool is_re
 // no active transfer
 void IDmaBe::enqueue_transfer(IdmaTransfer *transfer)
 {
-    this->trace.msg(vp::Trace::LEVEL_TRACE, "Queueing burst (burst: %p, src: 0x%x, dst: 0x%x, size: 0x%x)\n",
-        transfer, transfer->src, transfer->dst, transfer->size);
-
-    // Push the transfer into the queue, we will need it later when the bursts are coming back
-    // from memory. We will remove it from the queue when the transfer is fully done
-    this->transfer_queue.push(transfer);
-
-    // Extract information abouth the transfer
-    this->current_transfer = transfer;
-    this->current_transfer_size = transfer->size;
-    transfer->ack_size = transfer->size;
-    this->current_transfer_src = transfer->src;
-    this->current_transfer_dst = transfer->dst;
-    this->current_transfer_src_be = this->get_be_consumer(transfer->src, transfer->size, true);
-    this->current_transfer_dst_be = this->get_be_consumer(transfer->dst, transfer->size, false);
-
-    this->fsm_event.enqueue();
+    this->trace.msg(vp::Trace::LEVEL_TRACE, "Queueing burst (burst: %p, src: 0x%llx, dst: 0x%llx, size: 0x%x) | regulation_queue depth = %d\n",
+        transfer, transfer->src, transfer->dst, transfer->size,  this->regulation_queue.size());
+
+    this->regulation_queue.push(transfer);
+    this->transfer_regulation_event.enqueue();
+}
+
+void IDmaBe::transfer_regulation_handler(vp::Block *__this, vp::ClockEvent *event)
+{
+    IDmaBe *_this = (IDmaBe *)__this;
+
+    _this->trace.msg(vp::Trace::LEVEL_TRACE, "[transfer_regulation_handler] invoke\n");
+
+    if (!_this->regulation_queue.empty())
+    {
+        IdmaTransfer *wait_txn = _this->regulation_queue.front();
+        IdmaBeConsumer *wait_txn_src_be = _this->get_be_consumer(wait_txn->src, wait_txn->size, true);
+
+        if (!_this->transfer_ack_queue.empty() && (_this->current_transfer_src_be != wait_txn_src_be))
+        {
+            _this->trace.msg(vp::Trace::LEVEL_TRACE, "[transfer_regulation_handler] Src Conflict\n");
+            return;
+        }
+
+         _this->trace.msg(vp::Trace::LEVEL_TRACE, "[transfer_regulation_handler] Forward Transfer\n");
+
+        // Push the transfer into the queue, we will need it later when the bursts are coming back
+        // from memory. We will remove it from the queue when the transfer is fully done
+        IdmaTransfer *transfer = wait_txn;
+        _this->regulation_queue.pop();
+        _this->transfer_queue.push(transfer);
+        _this->transfer_ack_queue.push(transfer);
+
+        // Extract information abouth the transfer
+        _this->current_transfer = transfer;
+        _this->current_transfer_size = transfer->size;
+        transfer->ack_size = transfer->size;
+        _this->current_transfer_src = transfer->src;
+        _this->current_transfer_dst = transfer->dst;
+        _this->current_transfer_src_be = _this->get_be_consumer(transfer->src, transfer->size, true);
+        _this->current_transfer_dst_be = _this->get_be_consumer(transfer->dst, transfer->size, false);
+
+        _this->fsm_event.enqueue();
+
+        if (!_this->regulation_queue.empty())
+        {
+            _this->transfer_regulation_event.enqueue();
+        }
+    }
 }
 
 
@@ -85,6 +118,22 @@ bool IDmaBe::can_accept_transfer()
     return this->current_transfer_size == 0;
 }
 
+#ifdef ENABLE_DMA_SIMPLE_COLLECTIVE_IMPLEMENTATION
+uint64_t IDmaBe::get_collective_type()
+{
+    return this->current_transfer->parent->collective_type;
+}
+
+uint16_t IDmaBe::get_collective_row_mask()
+{
+    return this->current_transfer->parent->collective_row_mask;
+}
+
+uint16_t IDmaBe::get_collective_col_mask()
+{
+    return this->current_transfer->parent->collective_col_mask;
+}
+#endif //ENABLE_DMA_SIMPLE_COLLECTIVE_IMPLEMENTATION
 
 
 void IDmaBe::fsm_handler(vp::Block *__this, vp::ClockEvent *event)
@@ -142,7 +191,7 @@ void IDmaBe::fsm_handler(vp::Block *__this, vp::ClockEvent *event)
 void IDmaBe::update()
 {
     // Check if any action should be taken in the next cycle from the FSM handler
-    this->fsm_event.enqueue();
+    this->transfer_regulation_event.enqueue();
 }
 
 
@@ -169,6 +218,11 @@ void IDmaBe::write_data(uint8_t *data, uint64_t size)
     transfer->dst += size;
     transfer->size -= size;
 
+    if (transfer->size == 0)
+    {
+        this->transfer_queue.pop();
+    }
+
     // And forward data.
     // Note that the source backend already checked that the destination was ready by calling
     // our is_ready_to_accept_data method
@@ -181,7 +235,7 @@ void IDmaBe::write_data(uint8_t *data, uint64_t size)
 void IDmaBe::ack_data(uint8_t *data, int size)
 {
     // Get the source backend protocol for the first transfer
-    IdmaTransfer *transfer = this->transfer_queue.front();
+    IdmaTransfer *transfer = this->transfer_ack_queue.front();
     IdmaBeConsumer *src_be = this->get_be_consumer(transfer->src, transfer->size, true);
 
     // And acknowledge the data to it so that the data can be freed
@@ -199,8 +253,12 @@ void IDmaBe::ack_data(uint8_t *data, int size)
         this->trace.msg(vp::Trace::LEVEL_TRACE, "Finished burst (transfer: %p)\n", transfer);
 
         // And if so, remove it and notify the middle end
-        this->transfer_queue.pop();
+        this->transfer_ack_queue.pop();
         this->me->ack_transfer(transfer);
+        if (this->transfer_ack_queue.empty())
+        {
+            this->transfer_regulation_event.enqueue();
+        }
     }
 }
 
diff --git a/pulp/idma/be/idma_be.hpp b/pulp/idma/be/idma_be.hpp
index 4e2d730..782dafd 100644
--- a/pulp/idma/be/idma_be.hpp
+++ b/pulp/idma/be/idma_be.hpp
@@ -185,6 +185,15 @@ public:
      * by the source backend protocol.
      */
     virtual void ack_data(uint8_t *data, int size) = 0;
+
+#ifdef ENABLE_DMA_SIMPLE_COLLECTIVE_IMPLEMENTATION
+    /**
+     * @brief Get collective operation of current transfer
+     */
+    virtual uint64_t get_collective_type() = 0;
+    virtual uint16_t get_collective_row_mask() = 0;
+    virtual uint16_t get_collective_col_mask() = 0;
+#endif //ENABLE_DMA_SIMPLE_COLLECTIVE_IMPLEMENTATION
 };
 
 
@@ -221,10 +230,16 @@ public:
     bool is_ready_to_accept_data() override;
     void write_data(uint8_t *data, uint64_t size) override;
     void ack_data(uint8_t *data, int size) override;
+#ifdef ENABLE_DMA_SIMPLE_COLLECTIVE_IMPLEMENTATION
+    uint64_t get_collective_type() override;
+    uint16_t get_collective_row_mask() override;
+    uint16_t get_collective_col_mask() override;
+#endif //ENABLE_DMA_SIMPLE_COLLECTIVE_IMPLEMENTATION
 
 private:
     // FSM handler, called to check if any action should be taken after something was updated
     static void fsm_handler(vp::Block *__this, vp::ClockEvent *event);
+    static void transfer_regulation_handler(vp::Block *__this, vp::ClockEvent *event);
     // Returne backend protocol corresponding to the specified range
     IdmaBeConsumer *get_be_consumer(uint64_t base, uint64_t size, bool is_read);
     // Pointer to middle-end, used to interact with it
@@ -233,6 +248,7 @@ private:
     vp::Trace trace;
     // Block FSM event, used to trigger all checks after something has been updated
     vp::ClockEvent fsm_event;
+    vp::ClockEvent transfer_regulation_event;
     // Current transfer being processed. This transfer is kept active until all bursts
     // have been delegated to backend protocols.
     IdmaTransfer *current_transfer;
@@ -253,7 +269,9 @@ private:
     IdmaBeConsumer *current_transfer_dst_be;
     // Queue of pending transfers, whose bursts have already been sent. They need to be kept
     // since we need transfer information when bursts are back from memory
+    std::queue<IdmaTransfer *> regulation_queue;
     std::queue<IdmaTransfer *> transfer_queue;
+    std::queue<IdmaTransfer *> transfer_ack_queue;
     // Backend for local area
     IdmaBeConsumer *loc_be_read;
     IdmaBeConsumer *loc_be_write;
diff --git a/pulp/idma/be/idma_be_axi.cpp b/pulp/idma/be/idma_be_axi.cpp
index 149cb7f..1f5fa9c 100644
--- a/pulp/idma/be/idma_be_axi.cpp
+++ b/pulp/idma/be/idma_be_axi.cpp
@@ -93,6 +93,18 @@ void IDmaBeAxi::reset(bool active)
         {
             this->pending_bursts.pop();
         }
+        while(this->write_axi_sending_bursts.size() > 0)
+        {
+            this->write_axi_sending_bursts.pop();
+        }
+        while(this->issued_axi_burst_order_list.size() > 0)
+        {
+            this->issued_axi_burst_order_list.pop();
+        }
+        while(this->OoO_responses_waiting_list.size() > 0)
+        {
+            this->OoO_responses_waiting_list.pop_front();
+        }
 
         // And put back them all as free
         for (vp::IoReq &req: this->bursts)
@@ -141,13 +153,18 @@ void IDmaBeAxi::enqueue_burst(uint64_t base, uint64_t size, bool is_write)
     req->set_addr(base);
     req->set_size(size);
 
+    IDmaBeAxiWriteBurstInfo info;
+    info.base = base;
+    info.size = size;
+
     this->pending_bursts.push(req);
+    this->write_axi_sending_bursts.push(info);
 
     // It case it is the first burst, set the pending base, this is used for writing bursts to know next
     // req address
-    if (this->pending_bursts.size() == 1)
+    if (this->write_axi_sending_bursts.size() == 1)
     {
-        this->current_burst_base = this->pending_bursts.front()->get_addr();
+        this->current_burst_base = this->write_axi_sending_bursts.front().base;
     }
 
     // And trigger the FSM in case it needs to be processed now
@@ -178,9 +195,16 @@ void IDmaBeAxi::send_read_burst_to_axi()
 
     // Reinit timings
     req->prepare();
+#ifdef ENABLE_DMA_SIMPLE_COLLECTIVE_IMPLEMENTATION
+    uint8_t * payload_ptr = req->get_payload();
+    payload_ptr[0] = (uint8_t) this->be->get_collective_type();
+    payload_ptr[1] = (uint8_t) this->be->get_collective_row_mask();
+    payload_ptr[2] = (uint8_t) this->be->get_collective_col_mask();
+#endif
 
     // Send to AXI interface
     vp::IoReqStatus status = this->ico_itf.req(req);
+    this->issued_axi_burst_order_list.push(req);
 
     if (status == vp::IoReqStatus::IO_REQ_OK)
     {
@@ -206,10 +230,55 @@ void IDmaBeAxi::read_handle_req_end(vp::IoReq *req)
 {
     // Remember at which timestamp the burst must be notified
     this->read_timestamps[req->id] = this->clock.get_cycles() + req->get_latency();
-    // Queue the requests, they will be notified in order.
-    this->read_waiting_bursts.push(req);
-    // Enqueue fsm event at desired timestamp in case the event is not already enqueued before
-    this->fsm_event.enqueue(std::max(req->get_latency(), (uint64_t)1));
+
+    // Push to OoO_responses_waiting_list
+    this->OoO_responses_waiting_list.push_back(req);
+
+    // Check OoO_responses_waiting_list and order them to read_waiting_bursts
+    std::list<vp::IoReq *>::iterator OoO_iter;
+    while(true){
+        if (this->issued_axi_burst_order_list.size() == 0)
+        {
+            if (this->OoO_responses_waiting_list.size() != 0)
+            {
+                this->trace.msg(vp::Trace::LEVEL_WARNING, "[iDMA ROB] OoO_responses_waiting_list has remaining entry but issued_axi_burst_order_list size is 0\n");
+            } else {
+                break;
+            }
+        }
+
+        if (this->OoO_responses_waiting_list.size() == 0)
+        {
+            break;
+        }
+
+        int matched = 0;
+        vp::IoReq *req_to_check = this->issued_axi_burst_order_list.front();
+        for (OoO_iter = this->OoO_responses_waiting_list.begin(); OoO_iter != this->OoO_responses_waiting_list.end(); ++OoO_iter)
+        {
+            vp::IoReq *req_responsed = *OoO_iter;
+            if (req_to_check == req_responsed)
+            {
+                matched = 1;
+                break;
+            }
+        }
+
+        if (matched == 0)
+        {
+            break;
+        }
+
+        this->issued_axi_burst_order_list.pop();
+        this->read_waiting_bursts.push(req_to_check);
+        this->OoO_responses_waiting_list.erase(OoO_iter);
+    }
+
+    // Enqueue FSM when read_waiting_bursts is not empty
+    if (this->read_waiting_bursts.size() != 0)
+    {
+        this->fsm_event.enqueue(1);
+    }
 }
 
 
@@ -263,7 +332,18 @@ void IDmaBeAxi::write_data(uint8_t *data, uint64_t size)
     vp::IoReq *req = new vp::IoReq();
 
     uint64_t base = this->current_burst_base;
+
+    //next burst base
     this->current_burst_base += size;
+    this->write_axi_sending_bursts.front().size -= size;
+    if (this->write_axi_sending_bursts.front().size == 0)
+    {
+        this->write_axi_sending_bursts.pop();
+        if (this->write_axi_sending_bursts.size() > 0)
+        {
+            this->current_burst_base = this->write_axi_sending_bursts.front().base;
+        }
+    }
 
     this->trace.msg(vp::Trace::LEVEL_TRACE, "Write data (req: %p, base: 0x%lx, size: 0x%lx)\n",
         req, base, size);
@@ -273,6 +353,12 @@ void IDmaBeAxi::write_data(uint8_t *data, uint64_t size)
     req->set_addr(base);
     req->set_size(size);
     req->set_data(data);
+#ifdef ENABLE_DMA_SIMPLE_COLLECTIVE_IMPLEMENTATION
+    uint8_t * payload_ptr = req->get_payload();
+    payload_ptr[0] = (uint8_t) this->be->get_collective_type();
+    payload_ptr[1] = (uint8_t) this->be->get_collective_row_mask();
+    payload_ptr[2] = (uint8_t) this->be->get_collective_col_mask();
+#endif //ENABLE_DMA_SIMPLE_COLLECTIVE_IMPLEMENTATION
 
     vp::IoReqStatus status = this->ico_itf.req(req);
     if (status == vp::IoReqStatus::IO_REQ_OK)
@@ -312,10 +398,6 @@ void IDmaBeAxi::write_handle_req_end(vp::IoReq *req)
     if (burst->get_size() == 0)
     {
         this->pending_bursts.pop();
-        if (this->pending_bursts.size() > 0)
-        {
-            this->current_burst_base = this->pending_bursts.front()->get_addr();
-        }
         this->free_bursts.push(burst);
         // Notify the backend since it may schedule another burst
         this->be->update();
@@ -357,29 +439,37 @@ void IDmaBeAxi::fsm_handler(vp::Block *__this, vp::ClockEvent *event)
 
     // In case we have pending read bursts waiting for pushing data, only do it if the backend
     // is ready to accept the data in case the destination is not ready
-    if (_this->read_waiting_bursts.size() != 0 && _this->be->is_ready_to_accept_data())
+    if (_this->read_waiting_bursts.size() != 0 )
     {
-        vp::IoReq *req = _this->read_waiting_bursts.front();
-
-        // Push the data only once the timestamp has expired to take into account the latency
-        // returned when the data was read
-        if (_this->read_timestamps[req->id] <= _this->clock.get_cycles())
+        if (_this->be->is_ready_to_accept_data())
         {
-            // Move the burst to a different queue so that we can free the request when it is
-            // acknowledge
-            _this->read_waiting_bursts.pop();
-            _this->read_bursts_waiting_ack.push(req);
-
-            // Send the data
-            _this->be->write_data(req->get_data(), req->get_size());
-
-            // Trigger again the FSM since we may continue with another transfer
-            _this->fsm_event.enqueue();
+            vp::IoReq *req = _this->read_waiting_bursts.front();
+
+            // Push the data only once the timestamp has expired to take into account the latency
+            // returned when the data was read
+            if (_this->read_timestamps[req->id] <= _this->clock.get_cycles())
+            {
+                // Move the burst to a different queue so that we can free the request when it is
+                // acknowledge
+                _this->read_waiting_bursts.pop();
+                _this->read_bursts_waiting_ack.push(req);
+
+                // Send the data
+                _this->be->write_data(req->get_data(), req->get_size());
+
+                // Trigger again the FSM since we may continue with another transfer
+                _this->fsm_event.enqueue();
+            }
+            else
+            {
+                // Otherwise check again when timetamp is reached
+                _this->fsm_event.enqueue(_this->read_timestamps[req->id] - _this->clock.get_cycles());
+            }
         }
         else
         {
-            // Otherwise check again when timetamp is reached
-            _this->fsm_event.enqueue(_this->read_timestamps[req->id] - _this->clock.get_cycles());
+            // Enqueue FSM when read_waiting_bursts is not empty
+            _this->fsm_event.enqueue(1);
         }
     }
 }
@@ -396,4 +486,4 @@ void IDmaBeAxi::update()
 bool IDmaBeAxi::is_empty()
 {
     return this->pending_bursts.empty();
-}
\ No newline at end of file
+}
diff --git a/pulp/idma/be/idma_be_axi.hpp b/pulp/idma/be/idma_be_axi.hpp
index 211ea51..3e21a6f 100644
--- a/pulp/idma/be/idma_be_axi.hpp
+++ b/pulp/idma/be/idma_be_axi.hpp
@@ -21,11 +21,17 @@
 #pragma once
 
 #include <vector>
+#include <list>
 #include <vp/vp.hpp>
 #include <vp/itf/io.hpp>
 #include "../idma.hpp"
 #include "idma_be.hpp"
 
+typedef struct IDmaBeAxiWriteBurstInfo {
+    uint64_t base;
+    uint64_t size;
+} IDmaBeAxiWriteBurstInfo;
+
 /**
  * @brief AXI backend
  *
@@ -105,7 +111,14 @@ private:
     // processed.
     std::queue<vp::IoReq *> pending_bursts;
 
+    // Queue of pending bursts for write axi transactions. This only contains write bursts. This is decoupled from pending_bursts
+    std::queue<IDmaBeAxiWriteBurstInfo> write_axi_sending_bursts;
+
     // Current base of the first transfer. This is when a chunk of data to be written is received
     // to know the base where it should be written.
     uint64_t current_burst_base;
+
+    // Track the orders of DMA issued requests, for dealing with OoO responses
+    std::queue<vp::IoReq *> issued_axi_burst_order_list;
+    std::list<vp::IoReq *> OoO_responses_waiting_list;
 };
diff --git a/pulp/idma/fe/idma_fe_xdma.cpp b/pulp/idma/fe/idma_fe_xdma.cpp
index 867c0e2..1c3611a 100644
--- a/pulp/idma/fe/idma_fe_xdma.cpp
+++ b/pulp/idma/fe/idma_fe_xdma.cpp
@@ -46,6 +46,12 @@ IDmaFeXdma::IDmaFeXdma(vp::Component *idma, IdmaTransferConsumer *me)
 
     // Declare offload master interface for granting blocked transfers
     idma->new_master_port("offload_grant", &this->offload_grant_itf, this);
+
+    // track transfer time
+    this->transfer_start_time = 0;
+    this->num_inflight_transfer = 0;
+    this->total_idma_used_time = 0;
+    this->TxnList = "";
 }
 
 
@@ -87,19 +93,20 @@ void IDmaFeXdma::offload_sync(vp::Block *__this, IssOffloadInsn<uint32_t> *insn)
             _this->reps.set(insn->arg_a);
             break;
         case 0b0000011:
-            _this->trace.msg(vp::Trace::LEVEL_TRACE, "Received dmcpy operation (config: 0x%lx, size: 0x%lx)\n",
-                insn->arg_b, insn->arg_a);
-            insn->result = _this->enqueue_copy(insn->arg_b, insn->arg_a, insn->granted);
+            _this->trace.msg(vp::Trace::LEVEL_TRACE, "Received dmcpy collectve operation (config: 0x%lx, size: 0x%lx)\n",
+                ((insn->opcode >> 20) & 0b11111), insn->arg_a);
+            insn->result = _this->enqueue_copy(0b00000, insn->arg_a, insn->granted, ((insn->opcode >> 20) & 0b11111));
             break;
         case 0b0000101:
-            // _this->trace.msg(vp::Trace::LEVEL_TRACE, "Received dmstat operation (status: 0x%lx)\n",
-            //     insn->arg_b);
-            insn->result = _this->get_status(insn->arg_b);
+            _this->collective_row_mask = (insn->arg_b) >> 0;
+            _this->collective_col_mask = (insn->arg_b) >> 16;
+            _this->trace.msg(vp::Trace::LEVEL_TRACE, "Received dmmask operation (row mask: 0x%lx, col mask: 0x%lx)\n", _this->collective_row_mask, _this->collective_col_mask);
+            insn->result = insn->arg_b;
             break;
         case 0b0000010:
             _this->trace.msg(vp::Trace::LEVEL_TRACE, "Received dmcpy operation (config: 0x%lx, size: 0x%lx)\n",
                 insn->arg_b, insn->arg_a);
-            insn->result = _this->enqueue_copy(insn->arg_b, insn->arg_a, insn->granted);
+            insn->result = _this->enqueue_copy(insn->arg_b, insn->arg_a, insn->granted, 0);
             break;
         case 0b0000100:
             // _this->trace.msg(vp::Trace::LEVEL_TRACE, "Received dmstat operation (status: 0x%lx)\n",
@@ -126,13 +133,30 @@ uint32_t IDmaFeXdma::get_status(uint32_t status)
 
 
 
-uint32_t IDmaFeXdma::enqueue_copy(uint32_t config, uint32_t size, bool &granted)
+uint32_t IDmaFeXdma::enqueue_copy(uint32_t config, uint32_t size, bool &granted, uint32_t collective_type)
 {
     // Allocate transfer ID
     uint32_t transfer_id = this->next_transfer_id.get();
     this->next_transfer_id.set(transfer_id + 1);
 
     this->trace.msg(vp::Trace::LEVEL_TRACE, "Allocated transfer ID (id: %d)\n", transfer_id);
+    std::stringstream ss;
+    if (config == 0)
+    {
+        ss << "| Txn " << transfer_id << " = {type: 1D, src: 0x" << std::hex << this->src.get() \
+        << ", dst: 0x" << std::hex << this->dst.get() << ", size: 0x"<< std::hex << size << " }";
+    } else {
+        ss << "| Txn " << transfer_id << " = {type: 2D, src: 0x" << std::hex << this->src.get() << ", src_stride: 0x" << std::hex << this->src_stride.get() \
+        << ", dst: 0x" << std::hex << this->dst.get() << ", dst_stride: 0x" << std::hex << this->dst_stride.get() \
+        << ", repeats: 0x" << std::hex << this->reps.get() << ", size: 0x"<< std::hex << size << " }";
+    }
+
+    this->TxnList += ss.str();
+    if (this->num_inflight_transfer == 0)
+    {
+        this->transfer_start_time = this->time.get_time();
+    }
+    this->num_inflight_transfer += 1;
 
     // Allocate a new transfer and fill it from registers
     IdmaTransfer *transfer = new IdmaTransfer();
@@ -143,6 +167,11 @@ uint32_t IDmaFeXdma::enqueue_copy(uint32_t config, uint32_t size, bool &granted)
     transfer->dst_stride = this->dst_stride.get();
     transfer->reps = this->reps.get();
     transfer->config = config;
+#ifdef ENABLE_DMA_SIMPLE_COLLECTIVE_IMPLEMENTATION
+    transfer->collective_type = collective_type;
+    transfer->collective_row_mask = this->collective_row_mask;
+    transfer->collective_col_mask = this->collective_col_mask;
+#endif //ENABLE_DMA_SIMPLE_COLLECTIVE_IMPLEMENTATION
 
     // Check if middle end can accept a new transfer
     if (this->me->can_accept_transfer())
@@ -170,6 +199,13 @@ uint32_t IDmaFeXdma::enqueue_copy(uint32_t config, uint32_t size, bool &granted)
 void IDmaFeXdma::ack_transfer(IdmaTransfer *transfer)
 {
     this->completed_id.inc(1);
+    this->num_inflight_transfer -= 1;
+    if (this->num_inflight_transfer == 0)
+    {
+        this->total_idma_used_time += (this->time.get_time() - this->transfer_start_time)/1000;
+        this->trace.msg("[iDMA] Finished : %0d ns ---> %0d ns | period = %0d ns | runtime = %0d ns %s\n", (this->transfer_start_time/1000), (this->time.get_time()/1000), (this->time.get_time() - this->transfer_start_time)/1000, this->total_idma_used_time, this->TxnList.c_str());
+        this->TxnList = "";
+    }
     delete transfer;
 }
 
diff --git a/pulp/idma/fe/idma_fe_xdma.hpp b/pulp/idma/fe/idma_fe_xdma.hpp
index 300ef51..bc09050 100644
--- a/pulp/idma/fe/idma_fe_xdma.hpp
+++ b/pulp/idma/fe/idma_fe_xdma.hpp
@@ -20,6 +20,9 @@
 
 #pragma once
 
+#include <string>
+#include <iostream>
+#include <sstream>
 #include <vp/vp.hpp>
 #include <cpu/iss/include/offload.hpp>
 #include <vp/register.hpp>
@@ -51,7 +54,7 @@ private:
     // Method for offload interface, called when the core is offloading an xdma instruction
     static void offload_sync(vp::Block *__this, IssOffloadInsn<uint32_t> *insn);
     // Enqueue a transfer using the current values of the registers
-    uint32_t enqueue_copy(uint32_t config, uint32_t size, bool &granted);
+    uint32_t enqueue_copy(uint32_t config, uint32_t size, bool &granted, uint32_t collective_type);
     // Return status
     uint32_t get_status(uint32_t status);
 
@@ -82,4 +85,15 @@ private:
     vp::Signal<bool> do_transfer_grant;
     // In case a transfer was blocked, gives the transfer which was blocked
     IdmaTransfer *stalled_transfer;
+#ifdef ENABLE_DMA_SIMPLE_COLLECTIVE_IMPLEMENTATION
+    // Transfer collective
+    uint16_t collective_row_mask;
+    uint16_t collective_col_mask;
+#endif //ENABLE_DMA_SIMPLE_COLLECTIVE_IMPLEMENTATION
+
+    //track iDMA transfer time
+    int64_t transfer_start_time;
+    int64_t num_inflight_transfer;
+    int64_t total_idma_used_time;
+    std::string TxnList;
 };
diff --git a/pulp/idma/idma.hpp b/pulp/idma/idma.hpp
index 0a25bcb..cb2b021 100644
--- a/pulp/idma/idma.hpp
+++ b/pulp/idma/idma.hpp
@@ -23,6 +23,8 @@
 #include <vector>
 #include <vp/vp.hpp>
 
+#define ENABLE_DMA_SIMPLE_COLLECTIVE_IMPLEMENTATION
+
 
 
 /**
@@ -47,6 +49,12 @@ public:
     uint64_t reps;
     // Transfer config
     uint64_t config;
+#ifdef ENABLE_DMA_SIMPLE_COLLECTIVE_IMPLEMENTATION
+    // Transfer collective type
+    uint64_t collective_type;
+    uint16_t collective_row_mask;
+    uint16_t collective_col_mask;
+#endif //ENABLE_DMA_SIMPLE_COLLECTIVE_IMPLEMENTATION
 
     // Free rom for additional information
     std::vector<uint64_t> data;
diff --git a/pulp/idma/idma_functional.cpp b/pulp/idma/idma_functional.cpp
index e402244..d478579 100644
--- a/pulp/idma/idma_functional.cpp
+++ b/pulp/idma/idma_functional.cpp
@@ -181,7 +181,7 @@ vp::IoReqStatus IDma::req(vp::Block *__this, vp::IoReq *req)
     uint8_t *data = req->get_data();
     uint64_t size = req->get_size();
 
-    _this->trace.msg("IDma access (offset: 0x%x, size: 0x%x, is_write: %d)\n", offset, size, req->get_is_write());
+    _this->trace.msg("IDma access (offset: 0x%llx, size: 0x%x, is_write: %d)\n", offset, size, req->get_is_write());
 
     if (!req->get_is_write() && size == 8)
     {
diff --git a/pulp/snitch/sequencer.cpp b/pulp/snitch/sequencer.cpp
index 673529f..080f26f 100644
--- a/pulp/snitch/sequencer.cpp
+++ b/pulp/snitch/sequencer.cpp
@@ -21,7 +21,7 @@
  */
 
 // Temporary workaround to let this component include ISS headers
-#include <../../../../isa_snitch_rv32imfdvca.hpp>
+#include <../../../../isa_snitch_rv32imfdva.hpp>
 
 #include <vp/vp.hpp>
 #include <vp/itf/io.hpp>
diff --git a/pulp/snitch/snitch_cluster/snitch_cluster_peripheral_reg.hjson b/pulp/snitch/snitch_cluster/snitch_cluster_peripheral_reg.hjson
index e3a0c48..1e66943 100644
--- a/pulp/snitch/snitch_cluster/snitch_cluster_peripheral_reg.hjson
+++ b/pulp/snitch/snitch_cluster/snitch_cluster_peripheral_reg.hjson
@@ -411,6 +411,30 @@
             name: "ICACHE_PREFETCH_ENABLE",
             desc: "Enable instruction prefetching."
         }]
+    },
+    {
+        name: "NM_CONFIG",
+        desc: "Structured-sparsity format configuration register",
+        swaccess: "rw",
+        hwaccess: "hrw",
+        resval: "0",
+        fields: [
+            {
+                bits: "3:0",
+                name: "Format_N",
+                desc: "N value in N:M format (1, 2, 3, 4, 8)"
+            },
+            {
+                bits: "7:4",
+                name: "Format_M",
+                desc: "M value in N:M format (2, 4, 8, 16)"
+            },
+            {
+                bits: "31:8",
+                name: "RESERVED",
+                desc: "Reserved bits"
+            }
+        ]
     }
     ]
 }
diff --git a/pulp/snitch/snitch_core.py b/pulp/snitch/snitch_core.py
index 0bda5e4..186525b 100644
--- a/pulp/snitch/snitch_core.py
+++ b/pulp/snitch/snitch_core.py
@@ -86,20 +86,29 @@ class Snitch(cpu.iss.riscv.RiscvCommon):
     def __init__(self,
             parent,
             name,
-            isa: str='rv32imafdc',
+            isa: str='rv32imafd',
             misa: int=None,
             binaries: list=[],
             fetch_enable: bool=False,
             boot_addr: int=0,
             inc_spatz: bool=False,
+            spatz_num_vlsu: int=4,
+            spatz_num_fpu: int=4,
+            spatz_vlsu_bw: int=32,
+            spatz_vreg_gather_eff: float=1.0,
             core_id: int=0,
             htif: bool=False):
 
         isa_instance = isa_instances.get(isa)
 
         if isa_instances.get(isa) is None:
-            isa_instance = cpu.iss.isa_gen.isa_riscv_gen.RiscvIsa("snitch_" + isa, isa,
-                extensions=[ Rv32ssr(), Rv32frep(), Xdma(), Xf16(), Xf16alt(), Xf8(), Xfvec(), Xfaux() ] )
+            if inc_spatz:
+                isa_instance = cpu.iss.isa_gen.isa_riscv_gen.RiscvIsa("snitch_" + isa, isa,
+                    extensions=[ Rv32ssr(), Rv32frep(), Xdma(), Xf16(), Xf16alt(), Xf8(), Xfvec(), Xfaux(), Rv32redmule(), Rv32v() ] )
+            else:
+                isa_instance = cpu.iss.isa_gen.isa_riscv_gen.RiscvIsa("snitch_" + isa, isa,
+                    extensions=[ Rv32ssr(), Rv32frep(), Xdma(), Xf16(), Xf16alt(), Xf8(), Xfvec(), Xfaux(), Rv32redmule() ] )
+                pass
             add_latencies(isa_instance)
             isa_instances[isa] = isa_instance
 
@@ -146,17 +155,25 @@ class Snitch(cpu.iss.riscv.RiscvCommon):
             self.add_sources([
                 "cpu/iss/src/spatz.cpp",
             ])
+            self.add_c_flags(['-DCONFIG_GVSOC_ISS_INC_SPATZ=1'])
+            self.add_c_flags([f'-DCONFIG_GVSOC_ISS_SPATZ_VLSU={spatz_num_vlsu}'])
+            self.add_c_flags([f'-DCONFIG_GVSOC_ISS_SPATZ_FPU={spatz_num_fpu}'])
+            self.add_c_flags([f'-DCONFIG_GVSOC_ISS_SPATZ_VLSU_BW={spatz_vlsu_bw}'])
+            self.add_c_flags([f'-DCONFIG_GVSOC_ISS_SPATZ_VREG_GATHER_EFF={spatz_vreg_gather_eff}'])
 
     def o_BARRIER_REQ(self, itf: gvsoc.systree.SlaveItf):
         self.itf_bind('barrier_req', itf, signature='wire<bool>')
 
+    def o_REDMULE(self, itf: gvsoc.systree.SlaveItf):
+        self.itf_bind('redmule_itf', itf, signature='io')
+
 
 class SnitchBare(cpu.iss.riscv.RiscvCommon):
 
     def __init__(self,
             parent,
             name,
-            isa: str='rv32imafdc',
+            isa: str='rv32imafd',
             misa: int=None,
             binaries: list=[],
             fetch_enable: bool=False,
@@ -197,19 +214,28 @@ class Snitch_fp_ss(cpu.iss.riscv.RiscvCommon):
     def __init__(self,
             parent,
             name,
-            isa: str='rv32imafdc',
+            isa: str='rv32imafd',
             misa: int=None,
             binaries: list=[],
             fetch_enable: bool=False,
             boot_addr: int=0,
             inc_spatz: bool=False,
+            spatz_num_vlsu: int=4,
+            spatz_num_fpu: int=4,
+            spatz_vlsu_bw: int=32,
+            spatz_vreg_gather_eff: float=1.0,
             core_id: int=0,
             timed: bool=False,
             htif: bool=False):
 
-
-        isa_instance = cpu.iss.isa_gen.isa_riscv_gen.RiscvIsa("snitch_fp_ss_" + isa, isa,
-            extensions=[ Rv32ssr(), Rv32frep(), Xdma(), Xf16(), Xf16alt(), Xf8(), Xfvec(), Xfaux() ] )
+        if inc_spatz:
+            isa_instance = cpu.iss.isa_gen.isa_riscv_gen.RiscvIsa("snitch_fp_ss_" + isa, isa,
+                extensions=[ Rv32ssr(), Rv32frep(), Xdma(), Xf16(), Xf16alt(), Xf8(), Xfvec(), Xfaux(), Rv32redmule(), Rv32v() ] )
+        else:
+            isa_instance = cpu.iss.isa_gen.isa_riscv_gen.RiscvIsa("snitch_fp_ss_" + isa, isa,
+                extensions=[ Rv32ssr(), Rv32frep(), Xdma(), Xf16(), Xf16alt(), Xf8(), Xfvec(), Xfaux(), Rv32redmule() ] )
+            pass
+            
 
         add_latencies(isa_instance)
 
@@ -254,6 +280,11 @@ class Snitch_fp_ss(cpu.iss.riscv.RiscvCommon):
             self.add_sources([
                 "cpu/iss/src/spatz.cpp",
             ])
+            self.add_c_flags(['-DCONFIG_GVSOC_ISS_INC_SPATZ=1'])
+            self.add_c_flags([f'-DCONFIG_GVSOC_ISS_SPATZ_VLSU={spatz_num_vlsu}'])
+            self.add_c_flags([f'-DCONFIG_GVSOC_ISS_SPATZ_FPU={spatz_num_fpu}'])
+            self.add_c_flags([f'-DCONFIG_GVSOC_ISS_SPATZ_VLSU_BW={spatz_vlsu_bw}'])
+            self.add_c_flags([f'-DCONFIG_GVSOC_ISS_SPATZ_VREG_GATHER_EFF={spatz_vreg_gather_eff}'])
 
     def o_BARRIER_REQ(self, itf: gvsoc.systree.SlaveItf):
         self.itf_bind('barrier_req', itf, signature='wire<bool>')
@@ -266,7 +297,7 @@ class Spatz(cpu.iss.riscv.RiscvCommon):
     def __init__(self,
             parent,
             name,
-            isa: str='rv32imafdc',
+            isa: str='rv32imafd',
             misa: int=None,
             binaries: list=[],
             fetch_enable: bool=False,
diff --git a/pulp/snitch/snitch_isa.py b/pulp/snitch/snitch_isa.py
index 0f1fea3..956ccb9 100644
--- a/pulp/snitch/snitch_isa.py
+++ b/pulp/snitch/snitch_isa.py
@@ -28,7 +28,7 @@ class Xdma(IsaSubset):
             Instr('dmstr',     Format_R  ,   '0000110 ----- ----- 000 00000 0101011'),
             Instr('dmrep',     Format_R  ,   '0000111 ----- ----- 000 00000 0101011'),
             Instr('dmcpy',     Format_R  ,   '0000011 ----- ----- 000 ----- 0101011'),
-            Instr('dmstat',    Format_R  ,   '0000101 ----- ----- 000 ----- 0101011'),
+            Instr('dmmask',    Format_R  ,   '0000101 ----- ----- 000 ----- 0101011'),
             Instr('dmcpyi',    Format_I1U,   '0000010 ----- ----- 000 ----- 0101011'),
             Instr('dmstati',   Format_I1U,   '0000100 ----- ----- 000 ----- 0101011'),
         ])
@@ -100,3 +100,19 @@ class Rv32ssr(IsaSubset):
             Instr('scfgr', Format_SCFGR, '0000000----- 00001 001 ---- -0101011', tags=["ssr", 'nseq', 'fp_op']),
             Instr('scfgw', Format_SCFGW, '0000000----- ----- 010 0000 00101011', tags=["ssr", 'nseq', 'fp_op']),
         ])
+
+Format_MARITH = [
+    InReg (0, Range(15, 5)),
+    InReg (1, Range(20, 5)),
+    InReg (2, Range(27, 5)),
+    UnsignedImm(0, Range(7, 8)),
+]
+
+
+class Rv32redmule(IsaSubset):
+
+    def __init__(self):
+        super().__init__(name='redmule', instrs=[
+            Instr('mcnfig', Format_R     ,'0000000 ----- ----- 000 00000 0001010'),
+            Instr('marith', Format_MARITH,'-----00 ----- ----- --- ----- 0101010'),
+        ])
